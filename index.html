<!DOCTYPE html>
<html>

<head>
    <title>Brendan's PhD Thesis</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/theme/black.min.css">
    <link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <section>
                    <h2>Human Digitization</h2>
                </section>

                <section>
                    <ul>
                        <li>A number of tasks relate to human digitization</li>
                        <li>I mention a few of them below</li>
                    </ul>
                </section>

                <section>
                    <h3>Facial Reenactment</h3>
                </section>

                <section>
                    <ul>
                        <li>Facial reenacment involves learning the shape and appearance of a subject's face</li>
                        <li>We can then render the subject in novel poses and expressions using the learned representation</li>
                    </ul>
                </section>

                <section>
                    <ul>
                        <li>As an example, "NeRF for 4D Facial Avatars" makes use of a 3D morphable model (3DMM) to extend neural radiance fields (NeRF)</li>
                        <li>Vanilla NeRF conditions on position and view direction to predict coefficients for volumetric rendering</li>
                        <li>"NeRF for 4D Facial Avatars" extends NeRF to also condition on pose and expression from a 3DMM</li>
                        <li>After training, the model can render the subject in new facial poses and expressions</li>
                    </ul>
                </section>

                <section>
                    <h4>NeRF for 4D Facial Avatars</h4>
                    <video autoplay loop controls muted src="https://gafniguy.github.io/4D-Facial-Avatars/vid.mp4">
                    </video>
                    <footer>
                        Guy Gafni, Justus Thies, Michael Zollhöfer, and Matthias Nießner.
                        "NerFACE: Dynamic Neural Radiance Fields for Monocular 4D
                        Facial Avatar Reconstruction,"
                        <em>arXiv preprint arXiv:2012.03065</em>, 2020.
                    </footer>
                </section>

                <section>
                    <h3>Person and Scene Reconstruction</h3>
                </section>

                <section>
                    <ul>
                        <li>In 3D reconstruction we want to capture a 3D scene in high fidelity</li>
                        <li>Capturing a 3D scene requires multiple views</li>
                        <li>Foreground motion makes capturing scenes with people more difficult</li>
                    </ul>
                </section>

                <section>
                    <ul>
                        <li>Nerfies extend NeRF to handle scenes with mild deformations</li>
                        <li>They achieve this by introduction a deformation field that deforms input coordinates to a canonical NeRF</li>
                        <li>The resulting model can produce 3D reconstructions of humans from a selfie video, dubbed "Nerfies"</li>
                    </ul>
                </section>

                <section>
                    <h4>NeRFies</h4>
                    <div class="container">
                        <div>
                            <video autoplay controls muted loop>
                                <source src="https://storage.googleapis.com/nerfies-public/videos/steve.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div>
                            <video autoplay controls muted loop>
                                <source src="https://storage.googleapis.com/nerfies-public/videos/chair-tp.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div>
                            <video autoplay controls muted loop>
                                <source src="https://storage.googleapis.com/nerfies-public/videos/shiba.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <footer>
                        Keunhong Park, Utkarsh Sinha, Jonathan T. Barron,
                        Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and
                        Ricardo Martin-Brualla.
                        "Deformable Neural Radiance Fields,"
                        <em>arXiv preprint arXiv:2011.12948</em>, 2020.
                    </footer>
                </section>

                <section>
                    <h3>3D Statistical Model Acquisition</h3>
                </section>

                <section>
                    <ul>
                        <li>3D morphable models (3DMMs) are a 3D generative model for a known shape, e.g., for faces</li>
                        <li>3DMM creation proceeds by first collecting a dataset of 3D scans</li>
                        <li>We then align and register the 3D scans</li>
                        <li>We assume that the scans lie on a linear subspace</li>
                        <li>Hence, we form a 3DMM by fitting linear shape and expression bases to the aligned scans</li>
                    </ul>
                </section>

                <section>
                    <ul>
                        <li>FLAME is a 3DMM available for commercial use that is a middle ground between high and low-end 3DMMs</li>
                        <li>FLAME's linear shape space was learned from 3800 aligned scans of human heads</li>
                    </ul>
                </section>

                <section>
                    <h4>FLAME</h4>
                    <img data-src="https://flame.is.tue.mpg.de/uploads/ckeditor/pictures/195/content_FLAME_web_viewer.gif" alt="FLAME 3DMM GIF">
                    <footer>
                        Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero.
                        "Learning a model of facial shape and expression from 4D scans,"
                        <em>SIGGRAPH Asia</em>, 2017.
                    </footer>
                </section>
            </section>

            <section>
                <section>
                    <h2>Problem</h2>
                    <hr>
                    <h2>Slow Deployment</h2>
                </section>
                <section>
                    <h3>Potential Solution</h3>
                    <hr>
                    <h3>Speedup via Specialization</h3>
                </section>
            </section>

            <section>
                <section>
                    <h2>Problem</h2>
                    <hr>
                    <h2>Data Collection for Niche Tasks</h2>
                </section>
                <section>
                    <h3>Potential Solution</h3>
                    <hr>
                    <h3>Specialization from Generic models</h3>
                </section>
            </section>

            <section>
                <section>
                    <h2>Project Idea</h2>
                    <hr>
                    <h2>Building 3DMMs from NeRFies</h2>
                </section>

                <section>
                    <ul>
                        <li>This project falls under the "Data Collection for Niche Tasks" problem category</li>
                        <li>This is because we assume that we want to create new 3D statistical models of humans with specialized purposes</li>
                        <li>E.g., we may want to create a 3D statistical model with articulated eyeballs, or with hair as part of the head model</li>
                        <li>Existing methods would require a sophisticated 3D scan setup to collect training data for each new model</li>
                        <li>Furthermore, existing 3D scan datasets are not available for commercial use</li>
                    </ul>
                </section>

                <section>
                    <h3>Project Aim</h3>
                    <ul>
                        <li>This project's aim is to learn a 3D statistical model from video data instead of 3D scans</li>
                        <li>Learning from video data would give us the ability to build 3D statistical models from internet-scale data</li>
                        <li>We could build specialized datasets to train 3D statistical models on specific tasks with this plentiful video data</li>
                        <li>For this project we'll specifically focus on 3D Morphable Models (3DMMs) for human heads</li>
                        <li>But, the techniques developed should be general enough to create other model variants for different object types</li>
                    </ul>
                </section>

                <section>
                    <h3>Inverse Rendering</h3>
                    <ul>
                        <li>To go from video to 3D model we will use inverse rendering</li>
                        <li>Suppose we have an image I, and parameters $\theta$ describing a scene (e.g., normals, albedo, lighting, geometry, etc.)</li>
                        <li>Then we suppose that a differentiable renderer $R$, parametrized by $\theta$, formed our image as</li>
                        $$
                        I = R(\theta)
                        $$
                        <li>We can then optimize for the scene parameters by differentiating with respect to them to get</li>
                        $$
                        \frac{\partial I}{\partial \theta}
                        $$
                        <li>With suitable priors on $\theta$ we hope to use this setup to produce high quality scene parameters $\theta$</li>
                    </ul>
                </section>

                <section>
                    <h3>Inverting NeRF to get Face Geometry</h3>
                    <ul>
                        <li>We want to get face geometry $\theta$ from a set of video frames $\{I_j\}$</li>
                        <li>We propose to do so using Neural Radiance Fields (NeRF) as our differentiable renderer $R$</li>
                        <li>NeRF learns an implicit representation of the scene through a differentiable volumetric renderer</li>
                        <li>For each 3D position $\mathbf{x} = (x, y, z)$ in the scene, NeRF learns a scattering coefficient $\sigma_s(\mathbf{x})$</li>
                        <li>NeRF also learns sRGB colours $c(\mathbf{x}, \Theta)$ for position $\mathbf{x}$ and direction $\Theta$, a standin for irradiance</li>
                        <li>The resulting NeRF model can re-render scenes from novel views, as shown below</li>
                    </ul>
                    <div class="padding"></div>
                    <div class="container">
                        <div>
                            <video width="100%" height="100%" autoplay muted controls loop preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/orchid.jpg" aria-hidden="true">
                                <source src="https://storage.googleapis.com/nerf_data/website_renders/orchid.mp4" type="video/mp4" aria-hidden="true">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                        <div>
                            <video width="100%" height="100%" autoplay muted controls loop preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/redtoyota.jpg">
                                <source src="https://storage.googleapis.com/nerf_data/website_renders/redtoyota.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                        <div>
                            <video width="100%" height="100%" autoplay muted controls loop preload="metadata" poster="https://storage.googleapis.com/nerf_data/website_renders/images/pecanpie_200k_rgb.jpg">
                                <source src="https://storage.googleapis.com/nerf_data/website_renders/pecanpie_200k_rgb.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </section>

                <section>
                    <h3>Sampling Pseudo 3D Scan from NeRF</h3>
                    <ul>
                        <li>We are interested in using NeRF's learned scattering coefficients $\sigma_s$ and colours $c$</lI>
                        <li>Scattering coefficients $\sigma_s$ give a probability of light scattering at a 3D point</lI>
                        <li>This coefficient should be near 1 close to the surface</lI>
                        <li>Therefore, we could use $\sigma_s$ as a probability distribution describing the surface of a human head</lI>
                        <li>We can sample a point cloud close to the surface to replace 3D scans used in traditional 3DMM building pipelines</li>
                    </ul>
                </section>

                <section>
                    <h3>2D Video to 3DMM Overview</h3>
                    <img data-src="2d-video-to-3dmm-pipeline.svg" alt="2D Video to 3DMM Pipeline">
                </section>

                <section>
                    <h3>Potential Risks</h3>
                    <ul>
                        <li>Does NeRF learn a physically meaningful geometry?</li>
                        <li>May have to regularize NeRF to enforce this</li>
                        <li>We don't know the foreground / background status of NeRF 3D points a priori</li>
                        <li>May have to use a technique to segment out human head points from other scene points</li>
                    </ul>
                </section>
            </section>

            <section>
                <section>
                    <h2>Project Idea</h2>
                    <hr>
                    <h2>Faster Framerates with Neural Coded Exposure</h2>
                </section>

                <section>
                    <ul>
                        <li>Falls under the "Slow Deployment" problem category</li>
                    </ul>
                </section>
            </section>
        </div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/plugin/math/math.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/plugin/notes/notes.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.1.0/reveal.min.js"></script>
    <script>
        Reveal.initialize({
            plugins: [RevealMath, RevealNotes]
        });
    </script>
</body>

</html>
